{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from trains import Task\n",
    "from tqdm import tqdm\n",
    "sys.path.append('../src')\n",
    "import model\n",
    "import visualize as viz\n",
    "from data_loader import FarsightDataset, ToTensor\n",
    "from other_models.tiny_unet import UNet\n",
    "from utils import get_depth_dir, get_img_dir, get_dev, cfg\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "if cfg['verbose']:\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# task = Task.init(project_name='mde', task_name='test loop')\n",
    "# logger = task.get_logger()\n",
    "def weight_init(m):\n",
    "    \"\"\"\n",
    "    initialize weights of net to Kaiming and biases to zero,\n",
    "    since pytorch doesn't do that.\n",
    "\n",
    "    Usage: net.apply(weight_init)\n",
    "\n",
    "    Args:\n",
    "        m: layer from net\n",
    "\n",
    "    Returns:\n",
    "        None (initialized weights inplace).\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        m.weight.data\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity='relu')\n",
    "        nn.init.zeros_(m.bias)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    main train loop. all configurations are taken from the configs.yml file.\n",
    "    Returns:\n",
    "        trained net\n",
    "    \"\"\"\n",
    "    logger.info('getting params, dataloaders, etc...')\n",
    "    cfg_train = cfg['train']\n",
    "    epochs = cfg_train['epochs']\n",
    "    run_name = cfg_train['run_name']\n",
    "    print_every = cfg_train['print_every']\n",
    "    writer = SummaryWriter(comment=run_name)\n",
    "\n",
    "    train_loader, val_loader = get_loaders()\n",
    "    n_batches = len(train_loader)\n",
    "    # TODO: fix weird float32 requirement in conv2d to work with uint8. Quantization?\n",
    "    criterion, net, optimizer = get_net()\n",
    "\n",
    "    logger.info('got all params, starting train loop')\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        net.train()\n",
    "        with tqdm(total=n_batches, desc=f'Epoch {epoch + 1}/{epochs}', unit='batch') as pbar:\n",
    "            running_loss = 0.0\n",
    "            for data in train_loader:\n",
    "                # get the inputs; data is a list of [input images, depth maps]\n",
    "                img, gt_depth = data['image'], data['depth']\n",
    "                loss, pred_depth = step(criterion, img, gt_depth, net, optimizer)\n",
    "                loss_val = loss.item()\n",
    "                pbar.set_postfix(**{'loss (batch)': loss_val})\n",
    "                running_loss += loss_val\n",
    "                pbar.update()\n",
    "\n",
    "            if epoch % print_every == print_every - 1:\n",
    "                if cfg_train['val_round']:\n",
    "                    val_score = model.eval_net(net, val_loader, criterion, writer, epoch)\n",
    "                    # TODO: maybe add train_val\n",
    "                else:\n",
    "                    val_score = None\n",
    "                print_stats(net, data, epoch, val_score,\n",
    "                            pred_depth, running_loss, n_batches, writer)\n",
    "                running_loss = 0.0\n",
    "    print('Finished Training')\n",
    "    writer.close()\n",
    "    # if cfg_train['save']:\n",
    "    #     torch.save(net.state_dict(), os.path.join(cfg_train['foldername'],))\n",
    "    return gt_depth, pred_depth"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def step(criterion, img, gt_depth, net, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    pred_depth = net(img)\n",
    "    loss = criterion(pred_depth, gt_depth)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, pred_depth\n",
    "\n",
    "\n",
    "def print_stats(net, data, epoch, val_score,\n",
    "                pred_depth, running_loss, n_batches, writer):\n",
    "\n",
    "    print_every = cfg['train']['print_every']\n",
    "    writer.add_scalar('Loss/train', running_loss / (print_every * n_batches), epoch + 1)\n",
    "    logger.warning('\\ntrain loss: {}'.format(running_loss / (print_every * n_batches)))\n",
    "\n",
    "    if val_score is not None:\n",
    "        writer.add_scalar('Loss/val', val_score, epoch)\n",
    "        logger.warning('\\nValidation loss (metric?): {}'.format(val_score))\n",
    "\n",
    "    print_hist = cfg['evaluate']['hist']\n",
    "    if print_hist:\n",
    "        for tag, value in net.named_parameters():\n",
    "            tag = tag.replace('.', '/')\n",
    "            writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), epoch)\n",
    "            writer.add_histogram('grads/' + tag, value.grad.data.cpu().numpy(), epoch)\n",
    "        writer.add_histogram('values', pred_depth.detach().cpu().numpy(), epoch)\n",
    "    logger.info('logging images...')\n",
    "    fig = viz.show_batch({**data, 'pred': pred_depth.detach()})\n",
    "    fig.suptitle(f'step {epoch}', fontsize='xx-large')\n",
    "    writer.add_figure(tag='epoch/end', figure=fig, global_step=epoch)\n",
    "    writer.add_images('masks/gt', data['depth'].unsqueeze(1), epoch)\n",
    "    writer.add_images('masks/pred', pred_depth.unsqueeze(1), epoch)\n",
    "\n",
    "\n",
    "def get_net():\n",
    "    cfg_model = cfg['model']\n",
    "    model_name = cfg_model['name'].lower()\n",
    "    use_saved = cfg_model['use_saved']\n",
    "    if model_name == 'unet':\n",
    "        net = UNet()\n",
    "    elif model_name == 'toynet':\n",
    "        net = model.toyNet()\n",
    "    if use_saved:\n",
    "        logger.info('using saved model params')\n",
    "        path = cfg_model['path']\n",
    "        net.load_state_dict(torch.load(path))\n",
    "    net.to(device=get_dev())\n",
    "    print('using ', get_dev())\n",
    "    if cfg_model['weight_init']:\n",
    "        net.apply(weight_init)\n",
    "    # TODO: use loss in configs for loss.\n",
    "    loss_func = cfg_model['loss'].lower()\n",
    "    if loss_func.startswith('rmsle'):\n",
    "        logger.info('using rmsle')\n",
    "        criterion = model.RMSLELoss()\n",
    "    elif loss_func.startswith('mse'):\n",
    "        criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=cfg_model['lr'])\n",
    "    return criterion, net, optimizer\n",
    "\n",
    "\n",
    "def get_loaders():\n",
    "    cfg_train = cfg['train']\n",
    "    batch_size = cfg_train['batch_size']\n",
    "    batch_size_val = cfg['evaluate']['batch_size']\n",
    "    val_percent = cfg_train['val_percent']\n",
    "    subset_size = cfg_train['subset_size']\n",
    "    ds = FarsightDataset(transform=ToTensor())\n",
    "    if subset_size is not None:\n",
    "        ds = Subset(ds, range(subset_size))\n",
    "    n_val = int(len(ds) * val_percent)\n",
    "    n_train = len(ds) - n_val\n",
    "    train_split, val_split = random_split(ds,\n",
    "                                          [n_train, n_val],\n",
    "                                          generator=torch.Generator().manual_seed(42))\n",
    "                                          # TODO: check rnd. gen is consistent.\n",
    "                                          # TODO: make optional to use manual seed or random at some point. (same for DL?)\n",
    "    train_loader = DataLoader(train_split,\n",
    "                              shuffle=False,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=0)\n",
    "    val_loader = DataLoader(val_split,\n",
    "                            shuffle=False,\n",
    "                            batch_size=batch_size_val,\n",
    "                            num_workers=0)\n",
    "    return train_loader, val_loader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train()\n",
    "    # cProfile.run('train()', sort='tottime')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "torch",
   "language": "python",
   "display_name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}