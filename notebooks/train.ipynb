{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "sys.path.append('../src')\n",
    "\n",
    "# local imports.\n",
    "from utils import ROOT_DIR\n",
    "from data_loader import FarsightDataset, ToTensor\n",
    "import visualize as viz\n",
    "import model\n",
    "from utils import DATA_DIR, get_depth_dir, get_img_dir, get_dev\n",
    "from other_models.tiny_unet import UNet\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# task = Task.init(project_name='mde', task_name='test loop')\n",
    "# logger = task.get_logger()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from trains import Task\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight.data, nonlinearity='relu')\n",
    "#         torch.nn.init.normal_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epochs=2,\n",
    "          verbose=False,\n",
    "          batch_size=2,\n",
    "          val_percent=0.25):\n",
    "    \"\"\"\n",
    "    main training loop.\n",
    "    \"\"\"\n",
    "    print('started')\n",
    "    writer = SummaryWriter()\n",
    "    # create dataset\n",
    "    ds = FarsightDataset(img_dir=get_img_dir(),\n",
    "                         depth_dir=get_depth_dir(),\n",
    "                         transform=ToTensor())\n",
    "    minids = Subset(ds, range(2))\n",
    "    n_val = int(len(minids) * val_percent)\n",
    "    n_train = len(minids) - n_val\n",
    "    train, val = random_split(minids,\n",
    "                              [n_train, n_val],\n",
    "                              generator=torch.Generator().manual_seed(42))\n",
    "    train_loader = DataLoader(minids,\n",
    "                              shuffle=False,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=0)\n",
    "#     val_loader = DataLoader(val,\n",
    "#                             shuffle=False,\n",
    "#                             batch_size=batch_size,\n",
    "#                             num_workers=0)\n",
    "    # TODO: fix weird float32 requirement in conv2d to work with uint8. Quantization?\n",
    "    net = UNet()\n",
    "    net.to(device=get_dev())\n",
    "    print('using ', get_dev())\n",
    "    net.apply(weight_init)\n",
    "    num_batches = len(train_loader)\n",
    "    print('num_batches: ', num_batches)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "#     crap = torch.zeros((2,512,512),device=get_dev())\n",
    "    # main training loop.\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        net.train()\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(train_loader):\n",
    "                # get the inputs; data is a list of [input images, depth maps]\n",
    "                imgs, gt_depths = data['image'], data['depth']\n",
    "                optimizer.zero_grad()\n",
    "                # print('input shape {}, type: {}'.format(inputs.size(), inputs.dtype))\n",
    "                pred_depth = net(imgs)\n",
    "                #             print('out shape: {}, gt shape: {}'.format(outputs.size(), gt_depths.size()))\n",
    "                loss = criterion(pred_depth, gt_depths)\n",
    "#                 loss = criterion(pred_depth, crap)\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                pbar.update(imgs.shape[0])\n",
    "                # val statistics. check stats\n",
    "#                 if i == num_batches - 1:  # last batch\n",
    "                if epoch % 10 == 0:\n",
    "                    print(running_loss / num_batches)\n",
    "                    writer.add_scalar('Loss/train', running_loss / num_batches, epoch + 1)\n",
    "#                     print(running_loss /)\n",
    "                    # val scores\n",
    "#                     val_score = model.eval_net(net, val_loader, criterion, writer)\n",
    "#                     writer.add_scalar('Metric/test', val_score, epoch)\n",
    "#                     logging.info('Validation score (metric?): {}'.format(val_score))\n",
    "\n",
    "                    for tag, value in net.named_parameters():\n",
    "                        tag = tag.replace('.', '/')\n",
    "                        writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), epoch)\n",
    "                        writer.add_histogram('grads/' + tag, value.grad.data.cpu().numpy(), epoch)\n",
    "                    writer.add_histogram('values', pred_depth.detach().cpu().numpy(),epoch)\n",
    "                    fig = viz.show_batch({**data, 'pred': pred_depth.detach()})\n",
    "                    fig.suptitle(f'epoch {epoch}', fontsize='xx-large')\n",
    "#                     plt.show()\n",
    "#                     plt.title(f'epoch {epoch}')\n",
    "                    print('adding plot')\n",
    "                    writer.add_figure(tag='epoch/end', figure=fig, global_step=epoch)\n",
    "                    writer.add_images('images', imgs, epoch)\n",
    "                    writer.add_images('masks/true', gt_depths.unsqueeze(1), epoch)\n",
    "                    writer.add_images('masks/pred', pred_depth.unsqueeze(1), epoch)\n",
    "#             if verbose or epoch == epochs - 1:\n",
    "                #             viz.show_batch({**data, 'pred': outputs.detach()})\n",
    "#                 plt.show()\n",
    "    print('Finished Training')\n",
    "    writer.close()\n",
    "    return gt_depths, pred_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 2/2 [00:00<00:00, 10.76img/s, loss (batch)=0.243]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "using  cuda\n",
      "num_batches:  1\n",
      "0.24255569279193878\n",
      "batch size:  2\n",
      "adding plot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 2/2 [00:01<00:00,  1.25img/s, loss (batch)=0.243]\n",
      "Epoch 2/50: 100%|██████████| 2/2 [00:00<00:00,  5.88img/s, loss (batch)=0.206]\n",
      "Epoch 3/50: 100%|██████████| 2/2 [00:00<00:00, 10.69img/s, loss (batch)=0.186]\n",
      "Epoch 4/50: 100%|██████████| 2/2 [00:00<00:00, 10.60img/s, loss (batch)=0.17]\n",
      "Epoch 5/50: 100%|██████████| 2/2 [00:00<00:00, 10.87img/s, loss (batch)=0.159]\n",
      "Epoch 6/50: 100%|██████████| 2/2 [00:00<00:00, 10.69img/s, loss (batch)=0.151]\n",
      "Epoch 7/50: 100%|██████████| 2/2 [00:00<00:00, 10.64img/s, loss (batch)=0.145]\n",
      "Epoch 8/50: 100%|██████████| 2/2 [00:00<00:00, 10.66img/s, loss (batch)=0.14]\n",
      "Epoch 9/50: 100%|██████████| 2/2 [00:00<00:00, 10.76img/s, loss (batch)=0.134]\n",
      "Epoch 10/50: 100%|██████████| 2/2 [00:00<00:00, 10.33img/s, loss (batch)=0.128]\n",
      "Epoch 11/50: 100%|██████████| 2/2 [00:00<00:00, 10.83img/s, loss (batch)=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11948443949222565\n",
      "batch size:  2\n",
      "adding plot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 2/2 [00:01<00:00,  1.33img/s, loss (batch)=0.119]\n",
      "Epoch 12/50: 100%|██████████| 2/2 [00:00<00:00,  7.55img/s, loss (batch)=0.11]\n",
      "Epoch 13/50: 100%|██████████| 2/2 [00:00<00:00, 10.67img/s, loss (batch)=0.101]\n",
      "Epoch 14/50: 100%|██████████| 2/2 [00:00<00:00, 10.86img/s, loss (batch)=0.0946]\n",
      "Epoch 15/50: 100%|██████████| 2/2 [00:00<00:00, 10.23img/s, loss (batch)=0.0897]\n",
      "Epoch 16/50: 100%|██████████| 2/2 [00:00<00:00, 10.04img/s, loss (batch)=0.0848]\n",
      "Epoch 17/50: 100%|██████████| 2/2 [00:00<00:00, 10.09img/s, loss (batch)=0.0799]\n",
      "Epoch 18/50: 100%|██████████| 2/2 [00:00<00:00, 10.13img/s, loss (batch)=0.0753]\n",
      "Epoch 19/50: 100%|██████████| 2/2 [00:00<00:00,  9.86img/s, loss (batch)=0.0714]\n",
      "Epoch 20/50: 100%|██████████| 2/2 [00:00<00:00, 10.11img/s, loss (batch)=0.0678]\n",
      "Epoch 21/50: 100%|██████████| 2/2 [00:00<00:00, 10.26img/s, loss (batch)=0.0645]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06452243030071259\n",
      "batch size:  2\n",
      "adding plot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 2/2 [00:01<00:00,  1.12img/s, loss (batch)=0.0645]\n",
      "Epoch 22/50: 100%|██████████| 2/2 [00:00<00:00,  8.07img/s, loss (batch)=0.0617]\n",
      "Epoch 23/50: 100%|██████████| 2/2 [00:00<00:00, 10.12img/s, loss (batch)=0.0589]\n",
      "Epoch 24/50: 100%|██████████| 2/2 [00:00<00:00, 10.42img/s, loss (batch)=0.0565]\n",
      "Epoch 25/50: 100%|██████████| 2/2 [00:00<00:00, 10.46img/s, loss (batch)=0.0543]\n",
      "Epoch 26/50: 100%|██████████| 2/2 [00:00<00:00, 10.46img/s, loss (batch)=0.0522]\n",
      "Epoch 27/50: 100%|██████████| 2/2 [00:00<00:00, 10.60img/s, loss (batch)=0.0502]\n",
      "Epoch 28/50: 100%|██████████| 2/2 [00:00<00:00, 10.02img/s, loss (batch)=0.0484]\n",
      "Epoch 29/50: 100%|██████████| 2/2 [00:00<00:00, 10.18img/s, loss (batch)=0.0468]\n",
      "Epoch 30/50: 100%|██████████| 2/2 [00:00<00:00, 10.47img/s, loss (batch)=0.0453]\n",
      "Epoch 31/50: 100%|██████████| 2/2 [00:00<00:00, 10.41img/s, loss (batch)=0.0439]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.043906282633543015\n",
      "batch size:  2\n",
      "adding plot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 2/2 [00:01<00:00,  1.32img/s, loss (batch)=0.0439]\n",
      "Epoch 32/50: 100%|██████████| 2/2 [00:00<00:00,  9.21img/s, loss (batch)=0.0427]\n",
      "Epoch 33/50: 100%|██████████| 2/2 [00:00<00:00, 10.45img/s, loss (batch)=0.0415]\n",
      "Epoch 34/50: 100%|██████████| 2/2 [00:00<00:00, 10.00img/s, loss (batch)=0.0403]\n",
      "Epoch 35/50: 100%|██████████| 2/2 [00:00<00:00, 10.50img/s, loss (batch)=0.0393]\n",
      "Epoch 36/50: 100%|██████████| 2/2 [00:00<00:00, 10.36img/s, loss (batch)=0.0383]\n",
      "Epoch 37/50: 100%|██████████| 2/2 [00:00<00:00, 10.46img/s, loss (batch)=0.0374]\n",
      "Epoch 38/50: 100%|██████████| 2/2 [00:00<00:00, 10.37img/s, loss (batch)=0.0366]\n",
      "Epoch 39/50: 100%|██████████| 2/2 [00:00<00:00, 10.28img/s, loss (batch)=0.0359]\n",
      "Epoch 40/50: 100%|██████████| 2/2 [00:00<00:00, 10.62img/s, loss (batch)=0.0352]\n",
      "Epoch 41/50: 100%|██████████| 2/2 [00:00<00:00, 10.84img/s, loss (batch)=0.0346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0345703586935997\n",
      "batch size:  2\n",
      "adding plot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 2/2 [00:01<00:00,  1.41img/s, loss (batch)=0.0346]\n",
      "Epoch 42/50: 100%|██████████| 2/2 [00:00<00:00, 10.18img/s, loss (batch)=0.034]\n",
      "Epoch 43/50: 100%|██████████| 2/2 [00:00<00:00,  9.91img/s, loss (batch)=0.0336]\n",
      "Epoch 44/50: 100%|██████████| 2/2 [00:00<00:00, 10.46img/s, loss (batch)=0.0331]\n",
      "Epoch 45/50: 100%|██████████| 2/2 [00:00<00:00, 10.54img/s, loss (batch)=0.0326]\n",
      "Epoch 46/50: 100%|██████████| 2/2 [00:00<00:00, 10.58img/s, loss (batch)=0.0322]\n",
      "Epoch 47/50: 100%|██████████| 2/2 [00:00<00:00, 10.18img/s, loss (batch)=0.0319]\n",
      "Epoch 48/50: 100%|██████████| 2/2 [00:00<00:00, 10.52img/s, loss (batch)=0.0316]\n",
      "Epoch 49/50: 100%|██████████| 2/2 [00:00<00:00, 10.42img/s, loss (batch)=0.0312]\n",
      "Epoch 50/50: 100%|██████████| 2/2 [00:00<00:00, 10.43img/s, loss (batch)=0.0309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gt, pred = train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0080, 0.0080, 0.0080,  ..., 0.0120, 0.0120, 0.0120],\n",
       "         [0.0080, 0.0080, 0.0080,  ..., 0.0120, 0.0120, 0.0120],\n",
       "         [0.0080, 0.0080, 0.0080,  ..., 0.0120, 0.0120, 0.0080]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0080, 0.0080, 0.0080,  ..., 0.0120, 0.0120, 0.0120],\n",
       "         [0.0080, 0.0080, 0.0080,  ..., 0.0120, 0.0120, 0.0120],\n",
       "         [0.0080, 0.0080, 0.0080,  ..., 0.0120, 0.0120, 0.0120]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (laser-mde)",
   "language": "python",
   "name": "pycharm-c859ae41"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
